{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습 구현(매개변수값을 정하기 위해 경사하강법 사용---> 손실함수를 최소화하는 방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수 \n",
    "* 1. 평균 제곱 오차 (MSE)\n",
    "* 2. 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 알고리즘 구현\n",
    "* 1. 미니배치 --> 훈련데이터에서 무작위로 일부를 가져옴\n",
    "* 2. 기울기 산출  --> 각 가중치 매개변수의 기울기를 구한다.(기울기는 손실함수의 값을 작게하는 방향을 나타낸다.)\n",
    "* 3. 매개변수 갱신 --> 가중치 매개변수를 기울기 방향으로 갱신한다.(경사하강법 같은 방법을 사용한다.)\n",
    "* 4. 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mse구하는 함수 구현\n",
    "# y -> 예측값 , t -> 실제값\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "# 교차 엔트로피 오차 구하는 함수 구현\n",
    "# 컴퓨터에서는 log안에 0이 들어가면 오류가 발생하기 때문에 아주 작은 수를 더하여 오류를 해결한다.\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "t = [0,0,1,0,0,0,0,0,0]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0]\n",
    "\n",
    "\n",
    "print('mse : {:.4f}'.format(mean_squared_error(np.array(y),np.array(t))))\n",
    "print('cee : {:.4f}'.format(cross_entropy_error(np.array(y),np.array(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('C:/Users/174518/Desktop/c언어,파이선/파이썬 데이터/밑바닥부터시작하는딥러닝1/deep-learning-from-scratch-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 훈련 구현\n",
    "# 데이터의 양이 많으면 각각의 손실함수를 구하는 것은 힘드므로 미니배치를 통해 일부가 전체를 추정할 수 있게끔 만든다.\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w전체 훈련 데이터에서 무작위로 10장만 뽑기\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size,batch_size) # 전체 행에서 10개를 무작위로 뽑고 그 인덱스를 저장\n",
    "\n",
    "# 미니 배치(전체에서 10개만 추출)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터를 지원하는 교차엔트로피 오차 함수 구현하기 !!\n",
    "# 데이터가 하나인 경우와 배치로 묶여 입력될 경우 모두를 구현할 수 있도록 한다.\n",
    "\n",
    "def cross_entropy_error(y,t):  # y는 예측값 ,  t는 정답 레이블\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]  # 미니배치 개수 지정\n",
    "    return -np.sum(np.lo(y[np.arange(batch_size),t]))/batch_size\n",
    "# y의 값들이 원핫인코딩으로 인해 배열이 변경되고 정답 레이블만 가지고도 교차 엔트로피를 구할 수 있기 때문에 정답 레이블만을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[np.arange(batch_size),t]의 코드 간단한 예시 설명\n",
    "batch_size = 3\n",
    "row = np.arange(batch_size)\n",
    "t = [2,4,0]\n",
    "y = np.array([1,0,1,1,1,2,2,2,2,6,9,3,3,3,3]).reshape(3,5)\n",
    "y[row,t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 지표로 삼는 이유\n",
    "# 손실함수 => 연속적으로 변함\n",
    "# 정확도 => 불연속적으로 변함\n",
    "\n",
    "# 수치 미분을 하는 함수 구현하기!!\n",
    "# 함수의 기울기 출력\n",
    "def numeric_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*0.1\n",
    "\n",
    "x = np.arange(0.0,20.0,0.1)\n",
    "y = function_1(x)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분을 동시에 구할 수 있는 함수 구현하기\n",
    "# 기울기들을 출력한다.\n",
    "\n",
    "def numeric_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 구하기\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        # f(x-h)구하기\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_gradient(function_2,np.array([3.0,4.0]))  # x1과 x2 각각의 기울기 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수가 최소가 되게끔 하기 위해 경사하강법을 사용한다.\n",
    "# 학습률은 수동적으로 정해주어야 한다.\n",
    "\n",
    "# 0.01씩 계속 움직이는데 그 과정을 100번 반복한다.\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num = 100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        \n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x,np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법을 이용하여 x0^2 + x1^2 의 최소값을 구하라\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([3.0,4.0])\n",
    "x,x_history = gradient_descent(function_2,init_x,lr=0.01,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-5,5],[0,0],'--b')\n",
    "plt.plot([0,0],[-5,5],'--b')\n",
    "plt.plot(x_history[:,0],x_history[:,1],'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망에서의 기울기\n",
    "# 가중치 w를 변경했을 때 손실함수L이 얼마나 변화하는지를 나타낸다.\n",
    "\n",
    "# 신경망에서 기울기 구하는 코드 구현\n",
    "# class 사용하기!!\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "# 해당 경로에 있는 py파일 부르기\n",
    "from common.functions import *  # common파일의 functions.py를 import해서 모든 함수를 사용할 수 있게끔 만든다.\n",
    "from common.gradient import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class를 만들어서 사용하기\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNet()\n",
    "print('가중치 배열 : ',net.W)\n",
    "\n",
    "x = np.array([0.6,0.9])\n",
    "print('예측값 : ',net.predict(x))\n",
    "\n",
    "t = np.array([0,0,1]) # 정답 레이블\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return net.loss(x,t)\n",
    "dw = numerical_gradient(f,net.W)  # 손실함수의 기울기 구하기\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1단계 : 훈련데이터 미니배치   ---> 미니배치의 손실함수(교차엔트로피,mse) 값을 줄이는 것이 목표\n",
    "* 2단계 : 각 가중치 매개변수의 기울기를 구한다.\n",
    "* 3단계 : 매개변수 개선 ---> 경사하강법을 이용해 가중치 매개변수를 갱신한다.\n",
    "* 4단계 : 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망을 대상으로 신경망 구현  ---> 하나의 클래스로 구현\n",
    "\n",
    "import sys, os\n",
    "os.chdir('C:/Users/174518/Desktop/c언어,파이선/파이썬 데이터/밑바닥부터시작하는딥러닝1/deep-learning-from-scratch-master')\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import *\n",
    "\n",
    "class twolayernet:\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size,\n",
    "                weight_init_std=0.01):   # 입력층의 노드수, 은닉층의 노드수, 출력층의 노드수, 초기 가중치\n",
    "        # 가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self,x):  # x는 입력 값\n",
    "        W1,W2 = self.params['W1'],self.params['W2']\n",
    "        b1,b2 = self.params['b1'],self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1  # 입력값과 첫번째 가중치를 곱하고 편향 b를 더한 값\n",
    "        z1 = sigmoid(a1)        # a1의 값을 활성화 함수(시그모이드)에 넣어서 값을 출력한다.\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y   # 최종 예측값 출력\n",
    "    \n",
    "    # x : 입력값 , t : 정답 레이블\n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y,t)  # 손실함수(교차엔트로피 오차)를 구한다.\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)  # one-hot encoding 형식으로 나오기 때문에 1인 위치를 출력한다.\n",
    "        t = np.argmax(t,axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치를 이용해 mnist데이터를 2층 신경망에 집어넣어 구현\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10 #반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 할 개수\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 신경망에 적용하기\n",
    "network = twolayernet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 출력하기(랜덤으로 100개 출력)\n",
    "    batch_mask = np.random.choice(train_size,batch_size)  # 전체 train 데이터개수에서 100개를 랜덤으로 출력한다. (위치출력)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch,t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]  # 각 매개변수를 경사하강법을 이용해 갱신시킨다.(갱신을 batch_size만큼 반복)\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
